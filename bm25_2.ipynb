{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bm25_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5R1jrCVyFTF",
        "outputId": "b57b5e5d-9c97-487e-b883-df6edd53804c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.1-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rank_bm25) (1.19.5)\n",
            "Installing collected packages: rank-bm25\n",
            "Successfully installed rank-bm25-0.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install rank_bm25"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import tokenize\n",
        "from rank_bm25 import BM25Okapi\n",
        "from rank_bm25 import BM25L\n",
        "from rank_bm25 import BM25Plus\n",
        "from rank_bm25 import BM25\n",
        "\n",
        "# in the rank_BM25.py, the class BM25Adpt and BM25T are masked by #\n",
        "# we may activate the two classes where parameters k1 and b may be assigned with 1.2|2.0 (k1) and 0.75 (b)\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "import re \n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# step 1. load the hpo terms of all diseases/documents. each disease/document is long concated hpo term string\n",
        "    #First thing to do is create an instance of the BM25 class, which reads in a corpus of text and does some indexing on it:\n",
        "    # for each disease, concat all annotated hpo terms as the content for the disease/document.\n",
        "    # all diseases/documents make the corpus. \n",
        "\n",
        "corpus = [\n",
        "    \"Hello there good man!\",\n",
        "    \"It is quite windy in London!\",\n",
        "    \"How is the weather today?\",\n",
        "    \"who are you?\",\n",
        "    \"London is so windy and cold!\"\n",
        "]\n",
        "\n",
        "# step 2. tokenize all documents/diseases\n",
        "\n",
        "tokenized_corpus = [word_tokenize(doc) for doc in corpus ]\n",
        "\n",
        "'''\n",
        "# step 3. remove special characters\n",
        "\n",
        "def spl_chars_removal(lst):\n",
        "  lst1=list()\n",
        "  for token in lst:\n",
        "    str=\"\"\n",
        "    str = re.sub(r\"[^0-9a-zA-Z]\", \"\", token)\n",
        "    if not(str is \"\"):\n",
        "      lst1.append(str)      \n",
        "  return lst1\n",
        "\n",
        "print(tokenized_corpus)\n",
        "tokenized_corpus_v2 = [spl_chars_removal(lst) for lst in tokenized_corpus]\n",
        "tokenized_corpus_v3 = [lst for lst in tokenized_corpus_v2 if lst]\n",
        "print(tokenized_corpus_v3)\n",
        "del tokenized_corpus\n",
        "del tokenized_corpus_v2\n",
        "\n",
        "# step 4. remove stop words\n",
        "    # adding custom words to the pre-defined stop words list \n",
        "      ##  all_stopwords_gensim = STOPWORDS.union(set(['disease']))\n",
        "\n",
        "def stopwords_removal_gensim_custom(lst):\n",
        "  lst1=list()\n",
        "  for token in lst:\n",
        "    if not token in STOPWORDS:\n",
        "      lst1.append(token)  \n",
        "  return lst1\n",
        "\n",
        "tokenized_corpus_v4 = [stopwords_removal_gensim_custom(lst) for lst in tokenized_corpus_v3]\n",
        "tokenized_corpus_v5 = [lst for lst in tokenized_corpus_v4 if lst]\n",
        "print(tokenized_corpus_v5)\n",
        "del tokenized_corpus_v3\n",
        "del tokenized_corpus_v4\n",
        "\n",
        "# step 5. stem tokens\n",
        "\n",
        "ps = PorterStemmer()\n",
        "def stem_list(lst):\n",
        "  lst1 = list()\n",
        "  for token in lst:\n",
        "    lst1.append(ps.stem(token))\n",
        "  return lst1\n",
        "\n",
        "tokenized_corpus_v6 = [stem_list(lst) for lst in tokenized_corpus_v5]\n",
        "tokenized_corpus_v7 = [lst for lst in tokenized_corpus_v6 if lst]\n",
        "print(tokenized_corpus_v7)\n",
        "del tokenized_corpus_v6\n",
        "'''\n",
        "\n",
        "# step 6. create document indexes\n",
        "\n",
        "#bm25 = BM25Okapi(tokenized_corpus_v7)\n",
        "\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "#bm25 = BM25L(tokenized_corpus)\n",
        "\n",
        "#bm25 = BM25Plus(tokenized_corpus)\n",
        "\n",
        "  #Now that we've created our document indexes, we can give it queries and see which documents are the most relevant"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tH2uOnypyNQJ",
        "outputId": "4ba923f7-dd5f-4c6e-c07a-e9d42d243c4a"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Note that this package doesn't do any text preprocessing. If you want to do things like lowercasing, stopword removal, stemming, etc, you need to do it yourself."
      ],
      "metadata": {
        "id": "_PggLn4I2lzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The only requirements is that the class receives a list of lists of strings, which are the document tokens."
      ],
      "metadata": {
        "id": "hhm88TuQ2rTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"windy london\"\n",
        "tokenized_query = word_tokenize(query)\n",
        "#tokenized_query = spl_chars_removal(tokenized_query)\n",
        "#tokenized_query = stopwords_removal_gensim_custom(tokenized_query)\n",
        "#tokenized_query = stem_list(tokenized_query)\n",
        "#print(tokenized_query)\n",
        "\n",
        "doc_scores = bm25.get_scores(tokenized_query)\n",
        "print(doc_scores[0], doc_scores[1], doc_scores[2], doc_scores[3])\n",
        "print(doc_scores)\n",
        "print(len(doc_scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4MA1jNy2v8W",
        "outputId": "043314e3-c304-4583-e2cf-aad384e1051d"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0 0.30781371804464275 0.0 0.0\n",
            "[0.         0.30781372 0.         0.         0.30781372]\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bm25.get_top_n(tokenized_query, tokenized_corpus, n=3)\n",
        "# ['It is quite windy in London']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSLBP91Q3mXk",
        "outputId": "0b4e6605-b5ab-4b4e-d14f-eb587ec12868"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['London', 'is', 'so', 'windy', 'and', 'cold', '!'],\n",
              " ['It', 'is', 'quite', 'windy', 'in', 'London', '!'],\n",
              " ['who', 'are', 'you', '?']]"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    }
  ]
}